# -*- coding: utf-8 -*-
"""Data_generate.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dXlN4GUSdrFRosu_nApRaWGFJxaxSt5G
"""

import pandas as pd
import os
import ast
import re
import random
import json
random.seed(25)

from google.colab import drive
drive.mount('/content/drive')

json_brands = '/content/drive/MyDrive/Gen data/brands.json'
brands = []
with open(json_brands, 'r') as j:
     brands_json = json.loads(j.read())['RECORDS']
for i in range(len(brands_json)):
    brands.append(brands_json[i]['name'])
json_devices = '/content/drive/MyDrive/Gen data/devices.json'
devices = []
with open(json_devices, 'r') as j:
     devices_json = json.loads(j.read())['RECORDS']
for i in range(len(devices_json)):
    devices.append(devices_json[i]['name'])
files_btc = [f for f in os.listdir("/content/drive/MyDrive/Gen data/Data_btc") if
              os.path.isfile(os.path.join("/content/drive/MyDrive/Gen data/Data_btc", f))]
type_of_compare = ["DIF", "EQL", "SUP+", "SUP-", "SUP", "COM+", "COM-", "COM"]
resource = dict()
vocab = {'subject':set(),'object':set(),'aspect':dict(),'predicate':dict()}
data_frame = {"content": [], "comparative": [], "subject": [], "object": [], "aspect": [], "predicate": [], "label": [], "NER": []}
stat = [0,0,0,0,0,0,0,0]

def preprocessing(filename):
    data_row = []
    labeled_row = dict()
    data = []
    with (open('/content/drive/MyDrive/Gen data/Data_btc/' + filename, 'r', encoding="utf-8") as f):
        is_drop = False
        row = f.readline()
        while row != "":
            signal = ""
            split_row = row.split("\t")
            if len(split_row) > 1:
                r = split_row[1]
                if len(split_row) > 2:
                    signal = split_row[2]
            else:
                r = row
            if 'origin' in signal:
                is_drop = True
                row = f.readline()
                signal = ""
                continue
            elif 'quintuple' in signal:
                is_drop = False
                signal = ""
            if is_drop:
                row = f.readline()
                continue
            if '[origin' in r or '[quintuple]' in r or '[spellquintuple]' in r:
                r = r.replace('[origin]','')
                r = r.replace('[quintuple]','')
                r = r.replace('[spellquintuple]','')
                r = r.replace('[original]','')
            r = r.replace("\n", "")
            data_row.append(r)
            row = f.readline()
    f.close()
    del f
    j = 0
    for i in range(len(data_row) - 1, -1, -1):
        if data_row[i] == "":
            data_row.remove(data_row[i])
    for i in range(len(data_row) - 1):
        if re.match("^\{\"subject\": \[", data_row[i + 1]) and re.match("^\{\"subject\": \[", data_row[i]):
            row = eval(data_row[i + 1])
            labeled_row[data_row[j]].append(row)
        elif re.match("^\{\"subject\": \[", data_row[i + 1]):
            if data_row[i] not in labeled_row.keys():
                j = i
                row = eval(data_row[i + 1])
                labeled_row[data_row[j]] = [row]
            else:
                row = eval(data_row[i + 1])
                labeled_row[data_row[i]].append(row)
    for i in labeled_row.keys():
        for j in labeled_row[i]:
            subject = j['subject']
            object = j['object']
            aspect = j['aspect']
            predicate = j['predicate']
            s = ["" for k in range(len(subject))]
            o = ["" for k in range(len(object))]
            a = ["" for k in range(len(aspect))]
            p = ["" for k in range(len(predicate))]
            if len(subject) != 0:
                for k in range(len(subject)):
                    s[k] = subject[k][subject[k].rfind('&')+1:]
            if len(object) != 0:
                for k in range(len(object)):
                    o[k] = object[k][object[k].rfind('&')+1:]
            if len(aspect) != 0:
                for k in range(len(aspect)):
                    a[k] = aspect[k][aspect[k].rfind('&')+1:]
            if len(predicate) != 0:
                for k in range(len(predicate)):
                    p[k] = predicate[k][predicate[k].rfind('&')+1:]
            vocab['subject'].add(" ".join(s))
            vocab['object'].add(" ".join(o))
            joined_aspect = " ".join(a)
            joined_predicate = " ".join(p)
            if joined_aspect not in vocab['aspect'].keys():
                vocab['aspect'][joined_aspect] = set()
                vocab['aspect'][joined_aspect].add(j['label'])
            else:
                vocab['aspect'][joined_aspect].add(j['label'])
            if joined_predicate not in vocab['predicate'].keys():
                vocab['predicate'][joined_predicate] = set()
                vocab['predicate'][joined_predicate].add(j['label'])
            else:
                vocab['predicate'][joined_predicate].add(j['label'])
            stat[type_of_compare.index(j['label'])]+=1
    return labeled_row

stat = [0,0,0,0,0,0,0,0,0]
df_read_txt = pd.DataFrame(columns=["content", "comparative", "subject", "object", "aspect", "predicate", "label", "NER"])
def preprocessing_with_BIO(filename):
    data_frame = {"content": [], "comparative": [], "subject": [], "object": [], "aspect": [], "predicate": [], "label": [], "NER": []}
    data_row = []
    labeled_row = dict()
    data = []
    error = False
    with (open(filename, 'r', encoding="utf-8") as f):
        is_drop = False
        row = f.readline()
        while row != "":
            signal = ""
            split_row = row.split("\t")
            if len(split_row) > 1:
                r = split_row[1]
                if len(split_row) > 2:
                    signal = split_row[2]
            else:
                r = row
            if 'origin' in signal:
                is_drop = True
                row = f.readline()
                signal = ""
                continue
            elif 'quintuple' in signal:
                is_drop = False
                signal = ""
            if is_drop:
                row = f.readline()
                continue
            if '[origin' in r or '[quintuple]' in r or '[spellquintuple]' in r:
                r = r.replace('[origin]','')
                r = r.replace('[quintuple]','')
                r = r.replace('[spellquintuple]','')
                r = r.replace('[original]','')
            r = r.replace("\n", "")
            data_row.append(r)
            row = f.readline()
    f.close()
    del f
    j = 0
    for i in range(len(data_row) - 1, -1, -1):
        if data_row[i] == "":
            data_row.remove(data_row[i])
    for i in range(len(data_row) - 1):
        if (re.match("^\{\"subject\": \[", data_row[i + 1]) and re.match("^\{\"subject\": \[", data_row[i])) or (re.match("^\{\'subject\': \[", data_row[i + 1]) and re.match("^\{\'subject\': \[", data_row[i])):
            row = eval(data_row[i + 1])
            try:
                labeled_row[data_row[j]].append(row)
            except Exception as e:
                    print(filename,e,data_row[i+1])
        elif re.match("^\{\"subject\": \[", data_row[i + 1]) or re.match("^\{\'subject\': \[", data_row[i + 1]):
            if data_row[i] not in labeled_row.keys():
                j = i
                row = eval(data_row[i + 1])
                labeled_row[data_row[j]] = [row]
            else:
                try:
                    row = eval(data_row[i + 1])
                    labeled_row[data_row[i]].append(row)
                except Exception as e:
                    print(filename,e,data_row[i+1])
    for i in data_row:
        if re.match("^\{\"subject\": \[", i) is None and re.match("^\{\'subject\': \[", i) is None:
            data.append(i)
    del data_row
    del j
    del row
    for i in data:
        sentence_label = []
        key = 0
        subj = []
        obj = []
        asp = []
        pred = []
        lab = []
        w = i.split(" ")
        label = [0 for k in range(len(w))]
        if i in labeled_row.keys():
            key = 1
            for k in range(len(labeled_row.get(i))):
                label = [0 for k in range(len(w))]
                list_dct = labeled_row.get(i)
                if isinstance(list_dct[k]['subject'],str):
                        continue
                for j in list_dct[k].keys():
                    arr = list_dct[k].get(j)
                    consecutive = []
                    if j == "label":
                        continue
                    for e in range(len(arr)):
                        consecutive.append(int(arr[e][:arr[e].find("&")]))
                        arr[e] = re.sub("^[0-9]+&&", "", arr[e])
                    s = " ".join(arr)
                    list_dct[k][j] = s
                    for q in range(len(consecutive)):
                        if j == "subject":
                            try:
                                if label[consecutive[q]-1] == 0:
                                    if q == 0:
                                        label[consecutive[q] - 1] = 1
                                    else:
                                        label[consecutive[q] - 1] = 2
                            except:
                                error = True
                                break
                        elif j == "object":
                            try:
                                if label[consecutive[q]-1] == 0:
                                    if q == 0:
                                        label[consecutive[q] - 1] = 3
                                    else:
                                        label[consecutive[q] - 1] = 4
                            except:
                                error = True
                                break
                        elif j == "aspect":
                            try:
                                if label[consecutive[q]-1] == 0:
                                    if q == 0:
                                        label[consecutive[q] - 1] = 5
                                    else:
                                        label[consecutive[q] - 1] = 6
                            except:
                                error = True
                                break
                        elif j == "predicate":
                            try:
                                if label[consecutive[q]-1] == 0:
                                    if q == 0:
                                        label[consecutive[q] - 1] = 7
                                    else:
                                        label[consecutive[q] - 1] = 8
                            except:
                                error = True
                                break
                    try:
                        word_list = []
                        for m in consecutive:
                            word_list.append(w[m-1])
                        if ' '.join(word_list) == s:
                            pass
                        else:
                            error = True
                            break
                    except:
                        error = True
                        break
                if error:
                    break
                sentence_label.append(label)
                subj.append(list_dct[k]["subject"])
                obj.append(list_dct[k]["object"])
                pred.append(list_dct[k]["predicate"])
                asp.append(list_dct[k]["aspect"])
                lab.append(type_of_compare.index(list_dct[k]["label"]))
                stat[type_of_compare.index(list_dct[k]["label"])] += 1
        else:
            sentence_label = label
        if error == True:
            error = False
            continue
        data_frame["content"].append(i)
        data_frame["comparative"].append(key)
        data_frame["subject"].append(subj)
        data_frame["object"].append(obj)
        data_frame["predicate"].append(pred)
        data_frame["aspect"].append(asp)
        data_frame["label"].append(lab)
        data_frame["NER"].append(sentence_label)
        error = False
        lst_label = [lab.count(0),lab.count(1),lab.count(2),lab.count(3),lab.count(4),lab.count(5),lab.count(6),lab.count(7),lab.count(8)]
        lst_of_dict[lst_label.index(max(lst_label))][i] = labeled_row.get(i)
    return pd.DataFrame(data=data_frame)

for i in files_btc:
  df_read_txt = df_read_txt._append(preprocessing_with_BIO('/content/drive/MyDrive/Gen data/Data_ver1/'+i))
  new_source = preprocessing(i)
  for j in new_source.keys():
      resource[j] = new_source[j]
for i in brands:
    vocab['subject'].add(i)
    vocab['object'].add(i)
for i in devices:
    vocab['subject'].add(i)
    vocab['object'].add(i)

gen_data = dict()
generror = []
stat=[0,0,0,0,0,0,0,0]
def extract_pos(lst):
    return int(lst[0][:lst[0].find('&')]),int(lst[-1][:lst[-1].find('&')])

def reposition(element,diff):
    return str(int(element[:element.find('&')])+diff)+element[element.find('&'):]

def rewrite(thing,other,new_word_split,k,origin):
    new_obj_list = []
    for o in k[other]:
        new_obj_list.append(reposition(o,len(new_word_split)-len(origin)))
    return new_obj_list

def join_word(lst):
    n_lst = []
    for i in lst:
        n_lst.append(i[i.rfind('&')+1:])
    return ' '.join(n_lst)

def update_thing(list_of_json,sentence_split,new_word_split,gen_data,thing, origin):
    list_of_thing = ['subject','aspect','object','predicate']
    for k in list_of_json:
        if k[thing] == origin:
            position = extract_pos(origin)
            new_sentence_split = sentence_split[:position[0]-1] + new_word_split + sentence_split[position[1]:]
            for l in range(len(new_word_split)):
                new_word_split[l] = str(position[0]+l)+'&&'+new_word_split[l]
            break
    new_sentence = " ".join(new_sentence_split)
    s_split = new_sentence.split()
    if len(s_split) < 4:
        return
    new_list_of_json = []
    for k in list_of_json:
        new_dict = {"subject": k["subject"], "object": k["object"], "aspect": k["aspect"], "predicate": k['predicate'], 'label':k['label']}
        if k[thing] == origin:
            new_dict[thing] = new_word_split
        n_sub = s_sub = ''
        n_ob = s_ob = ''
        n_as = s_asp = ''
        n_pre = s_pred = ''
        if len(k['subject']) != 0:
            if 'subject' != thing:
                thing_pos = extract_pos(k['subject'])
                if (thing_pos[0] >= position[0] and thing_pos[1] <= position[1]) or \
                   (thing_pos[0] <= position[1] and thing_pos[0] >= position[0] and thing_pos[1] > position[1]) or \
                   (thing_pos[1] <= position[1] and thing_pos[1] >= position[0] and thing_pos[0] < position[0]):
                    new_dict['subject'] = []
                elif thing_pos[0] > position[1]:
                    new_dict['subject'] = rewrite(thing,'subject',new_word_split,k,origin)
                    n_sub = join_word(new_dict['subject'])
                    pos_sub = extract_pos(new_dict['subject'])
                    s_sub = ' '.join(s_split[pos_sub[0]-1:pos_sub[1]])
                elif thing_pos[0] <= position[0] and thing_pos[1] >= position[1]:
                    new_subject_list = []
                    for l in k['subject']:
                        new_subject_list.append(l)
                        if int(l[:l.find('&')]) == position[0]:
                            break
                    for l in range(1,len(new_word_split)):
                        new_subject_list.append(new_word_split[l])
                    original_subject_list = k['subject'][position[0]-thing_pos[0]:position[1] - thing_pos[1]]
                    for l in range(len(k['subject'])+position[1] - thing_pos[1],len(k['subject'])):
                        new_subject_list.append(reposition(k['subject'][l],len(new_word_split)-len(original_subject_list)))
                    new_dict['subject'] = new_subject_list
        if len(k['aspect']) != 0:
            if 'aspect' != thing:
                thing_pos = extract_pos(k['aspect'])
                if (thing_pos[0] >= position[0] and thing_pos[1] <= position[1]) or \
                   (thing_pos[0] <= position[1] and thing_pos[0] >= position[0] and thing_pos[1] > position[1]) or \
                   (thing_pos[1] <= position[1] and thing_pos[1] >= position[0] and thing_pos[0] < position[0]):
                    new_dict['aspect'] = []
                elif thing_pos[0] > position[1]:
                    new_dict['aspect'] = rewrite(thing,'aspect',new_word_split,k,origin)
                    n_as = join_word(new_dict['aspect'])
                    pos_asp = extract_pos(new_dict['aspect'])
                    s_asp = ' '.join(s_split[pos_asp[0]-1:pos_asp[1]])
                elif thing_pos[0] <= position[0] and thing_pos[1] >= position[1]:
                    new_subject_list = []
                    for l in k['aspect']:
                        new_subject_list.append(l)
                        if int(l[:l.find('&')]) == position[0]:
                            break
                    for l in range(1,len(new_word_split)):
                        new_subject_list.append(new_word_split[l])
                    original_subject_list = k['aspect'][position[0]-thing_pos[0]:position[1] - thing_pos[1]]
                    for l in range(len(k['aspect'])+position[1] - thing_pos[1],len(k['aspect'])):
                        new_subject_list.append(reposition(k['aspect'][l],len(new_word_split)-len(original_subject_list)))
                    new_dict['aspect'] = new_subject_list
        if len(k['object']) != 0:
            if 'object' != thing:
                thing_pos = extract_pos(k['object'])
                if (thing_pos[0] >= position[0] and thing_pos[1] <= position[1]) or \
                   (thing_pos[0] <= position[1] and thing_pos[0] >= position[0] and thing_pos[1] > position[1]) or \
                   (thing_pos[1] <= position[1] and thing_pos[1] >= position[0] and thing_pos[0] < position[0]):
                    new_dict['object'] = []
                elif thing_pos[0] > position[1]:
                    new_dict['object'] = rewrite(thing,'object',new_word_split,k,origin)
                    n_ob = join_word(new_dict['object'])
                    pos_obj = extract_pos(new_dict['object'])
                    s_ob = ' '.join(s_split[pos_obj[0]-1:pos_obj[1]])
                elif thing_pos[0] <= position[0] and thing_pos[1] >= position[1]:
                    new_subject_list = []
                    for l in k['object']:
                        new_subject_list.append(l)
                        if int(l[:l.find('&')]) == position[0]:
                            break
                    for l in range(1,len(new_word_split)):
                        new_subject_list.append(new_word_split[l])
                    original_subject_list = k['object'][position[0]-thing_pos[0]:position[1] - thing_pos[1]]
                    for l in range(len(k['object'])+position[1] - thing_pos[1],len(k['object'])):
                        new_subject_list.append(reposition(k['object'][l],len(new_word_split)-len(original_subject_list)))
                    new_dict['object'] = new_subject_list
        if len(k['predicate']) != 0:
            if 'predicate' != thing:
                thing_pos = extract_pos(k['predicate'])
                if (thing_pos[0] >= position[0] and thing_pos[1] <= position[1]) or \
                   (thing_pos[0] <= position[1] and thing_pos[0] >= position[0] and thing_pos[1] > position[1]) or \
                   (thing_pos[1] <= position[1] and thing_pos[1] >= position[0] and thing_pos[0] < position[0]):
                    new_sentence = ' '.join(sentence_split)
                    return
                elif thing_pos[0] > position[1]:
                    new_dict['predicate'] = rewrite(thing,'predicate',new_word_split,k,origin)
                    n_pre = join_word(new_dict['predicate'])
                    pos_pred = extract_pos(new_dict['predicate'])
                    s_pred = ' '.join(s_split[pos_pred[0]-1:pos_pred[1]])
                elif thing_pos[0] <= position[0] and thing_pos[1] >= position[1]:
                    new_subject_list = []
                    for l in k['predicate']:
                        new_subject_list.append(l)
                        if int(l[:l.find('&')]) == position[0]:
                            break
                    for l in range(1,len(new_word_split)):
                        new_subject_list.append(new_word_split[l])
                    original_subject_list = k['predicate'][position[0]-thing_pos[0]:position[1] - thing_pos[1]]
                    for l in range(len(k['predicate'])+position[1] - thing_pos[1],len(k['predicate'])):
                        new_subject_list.append(reposition(k['predicate'][l],len(new_word_split)-len(original_subject_list)))
                    new_dict['predicate'] = new_subject_list
        if n_sub == s_sub and n_ob == s_ob and n_as == s_asp and n_pre == s_pred:
            new_list_of_json.append(new_dict)
        else:
            generror.append(new_sentence+'\n'+new_dict.__str__())
            return
    if new_sentence not in gen_data.keys():
        gen_data[new_sentence] = []
        for i in range(len(new_list_of_json)):
            if new_list_of_json[i] not in gen_data[new_sentence]:
                gen_data[new_sentence].append(new_list_of_json[i])
        stat[type_of_compare.index(new_list_of_json[i]['label'])] += 1
        return new_sentence


for i in range(len(type_of_compare)):
    dict_of_sentence = dict()
    for j in resource.keys():
        for k in resource[j]:
            if k['label'] == type_of_compare[i]:
                if j not in dict_of_sentence.keys():
                    dict_of_sentence[j] = [k]
                else:
                    dict_of_sentence[j].append(k)
    list_of_pred = []
    for j in vocab['predicate'].keys():
        if type_of_compare[i] in vocab['predicate'][j]:
            list_of_pred.append(j)
    list_of_asp = []
    for j in vocab['aspect'].keys():
        if type_of_compare[i] in vocab['aspect'][j]:
            list_of_asp.append(j)
    while stat[i] <= 5000:
        sentence = random.choice(list(dict_of_sentence.keys()))
        sentence_split = sentence.split()
        list_of_json = dict_of_sentence[sentence]
        new_word = random.choice(list_of_pred)
        new_word = new_word.lower()
        new_sentence = update_thing(list_of_json, sentence_split, new_word.split(), gen_data, 'predicate',list_of_json[0]['predicate'])
        if new_sentence is not None:
            sentence = new_sentence
        k = 0
        try:
            while k < len(gen_data[sentence]):
                if len(gen_data[sentence][k]['aspect']) != 0:
                    new_word = random.choice(list_of_asp)
                    new_word = new_word.lower()
                    list_of_json = gen_data[sentence]
                    new_sentence = update_thing(list_of_json, sentence.split(), new_word.split(),gen_data,'aspect', gen_data[sentence][k]['aspect'])
                    if new_sentence is not None:
                        sentence = new_sentence
                k += 1
            k = 0
            while k < len(gen_data[sentence]):
                if len(gen_data[sentence][k]['subject']) != 0:
                    new_word = random.choice(list(vocab['subject']))
                    new_word = new_word.lower()
                    list_of_json = gen_data[sentence]
                    sentence = update_thing(list_of_json, sentence.split(), new_word.split(),gen_data,'subject',gen_data[sentence][k]['subject'])
                    if new_sentence is not None:
                        sentence = new_sentence
                k += 1
            k = 0
            while k < len(gen_data[sentence]):
                if len(gen_data[sentence][k]['object']) != 0:
                    new_word = random.choice(list(vocab['object']))
                    new_word = new_word.lower()
                    list_of_json = gen_data[sentence]
                    sentence = update_thing(list_of_json, sentence.split(), new_word.split(),gen_data,'object',gen_data[sentence][k]['object'])
                    if new_sentence is not None:
                        sentence = new_sentence
                k += 1
        except:
            continue

with open('/content/drive/MyDrive/Gen data/Generative/large.txt', 'w', encoding='utf-8') as f:
    for i in gen_data.keys():
        f.write(i+'\n')
        for j in gen_data[i]:
            f.write(str(j)+'\n')
        f.write('\n')

stat = [0,0,0,0,0,0,0,0,0]
usable_gen_data = preprocessing_with_BIO('/content/drive/MyDrive/Gen data/Generative/large.txt')

df_gen1_mono = pd.DataFrame(columns=["content", "comparative", "subject", "object", "aspect", "predicate", "label", "NER"])
df_gen1_multi = pd.DataFrame(columns=["content", "comparative", "subject", "object", "aspect", "predicate", "label", "NER"])
df_gen2_mono = pd.DataFrame(columns=["content", "comparative", "subject", "object", "aspect", "predicate", "label", "NER"])
df_gen2_multi = pd.DataFrame(columns=["content", "comparative", "subject", "object", "aspect", "predicate", "label", "NER"])
df_gen3_mono = pd.DataFrame(columns=["content", "comparative", "subject", "object", "aspect", "predicate", "label", "NER"])
df_gen3_multi = pd.DataFrame(columns=["content", "comparative", "subject", "object", "aspect", "predicate", "label", "NER"])
df_gen4_mono = pd.DataFrame(columns=["content", "comparative", "subject", "object", "aspect", "predicate", "label", "NER"])
df_gen4_multi = pd.DataFrame(columns=["content", "comparative", "subject", "object", "aspect", "predicate", "label", "NER"])
df_gen5_mono = pd.DataFrame(columns=["content", "comparative", "subject", "object", "aspect", "predicate", "label", "NER"])
df_gen5_multi = pd.DataFrame(columns=["content", "comparative", "subject", "object", "aspect", "predicate", "label", "NER"])
df_gen6_mono = pd.DataFrame(columns=["content", "comparative", "subject", "object", "aspect", "predicate", "label", "NER"])
df_gen6_multi = pd.DataFrame(columns=["content", "comparative", "subject", "object", "aspect", "predicate", "label", "NER"])
df_gen7_mono = pd.DataFrame(columns=["content", "comparative", "subject", "object", "aspect", "predicate", "label", "NER"])
df_gen7_multi = pd.DataFrame(columns=["content", "comparative", "subject", "object", "aspect", "predicate", "label", "NER"])
df_gen8_mono = pd.DataFrame(columns=["content", "comparative", "subject", "object", "aspect", "predicate", "label", "NER"])
df_gen8_multi = pd.DataFrame(columns=["content", "comparative", "subject", "object", "aspect", "predicate", "label", "NER"])

for i in range(len(usable_gen_data)):
    if len(usable_gen_data.iloc[i,6]) == 1 and 1 in usable_gen_data.iloc[i,6]:
        df_gen1_mono = df_gen1_mono._append(usable_gen_data.iloc[i])
    if len(usable_gen_data.iloc[i,6]) > 1 and 1 in usable_gen_data.iloc[i,6]:
        df_gen1_multi = df_gen1_multi._append(usable_gen_data.iloc[i])
    if len(usable_gen_data.iloc[i,6]) == 1 and 2 in usable_gen_data.iloc[i,6]:
        df_gen2_mono = df_gen2_mono._append(usable_gen_data.iloc[i])
    if len(usable_gen_data.iloc[i,6]) > 1 and 2 in usable_gen_data.iloc[i,6]:
        df_gen2_multi = df_gen2_multi._append(usable_gen_data.iloc[i])
    if len(usable_gen_data.iloc[i,6]) == 1 and 3 in usable_gen_data.iloc[i,6]:
        df_gen3_mono = df_gen3_mono._append(usable_gen_data.iloc[i])
    if len(usable_gen_data.iloc[i,6]) > 1 and 3 in usable_gen_data.iloc[i,6]:
        df_gen3_multi = df_gen3_multi._append(usable_gen_data.iloc[i])
    if len(usable_gen_data.iloc[i,6]) == 1 and 4 in usable_gen_data.iloc[i,6]:
        df_gen4_mono = df_gen4_mono._append(usable_gen_data.iloc[i])
    if len(usable_gen_data.iloc[i,6]) > 1 and 4 in usable_gen_data.iloc[i,6]:
        df_gen4_multi = df_gen4_multi._append(usable_gen_data.iloc[i])
    if len(usable_gen_data.iloc[i,6]) == 1 and 5 in usable_gen_data.iloc[i,6]:
        df_gen5_mono = df_gen5_mono._append(usable_gen_data.iloc[i])
    if len(usable_gen_data.iloc[i,6]) > 1 and 5 in usable_gen_data.iloc[i,6]:
        df_gen5_multi = df_gen5_multi._append(usable_gen_data.iloc[i])
    if len(usable_gen_data.iloc[i,6]) == 1 and 6 in usable_gen_data.iloc[i,6]:
        df_gen6_mono = df_gen6_mono._append(usable_gen_data.iloc[i])
    if len(usable_gen_data.iloc[i,6]) > 1 and 6 in usable_gen_data.iloc[i,6]:
        df_gen6_multi = df_gen6_multi._append(usable_gen_data.iloc[i])
    if len(usable_gen_data.iloc[i,6]) == 1 and 7 in usable_gen_data.iloc[i,6]:
        df_gen7_mono = df_gen7_mono._append(usable_gen_data.iloc[i])
    if len(usable_gen_data.iloc[i,6]) > 1 and 7 in usable_gen_data.iloc[i,6]:
        df_gen7_multi = df_gen7_multi._append(usable_gen_data.iloc[i])
    if len(usable_gen_data.iloc[i,6]) == 1 and 8 in usable_gen_data.iloc[i,6]:
        df_gen8_mono = df_gen8_mono._append(usable_gen_data.iloc[i])
    if len(usable_gen_data.iloc[i,6]) > 1 and 8 in usable_gen_data.iloc[i,6]:
        df_gen8_multi = df_gen8_multi._append(usable_gen_data.iloc[i])
print(len(df_gen1_mono),len(df_gen1_multi))

df_gen1_mono[df_gen1_mono.NER == '[]']

print(len(df_gen7_mono),len(df_gen7_multi))
print(len(df_gen8_mono),len(df_gen8_multi))

"""Ver 1"""

df_gen1 = df_gen1_mono.sample(85,random_state=10)._append(df_gen1_multi.sample(34,random_state=10))
df_gen2 = df_gen2_mono.sample(282,random_state=10)._append(df_gen2_multi.sample(113,random_state=10))
df_gen3 = df_gen3_mono.sample(91,random_state=10)._append(df_gen3_multi.sample(36,random_state=10))
df_gen4 = df_gen4_mono.sample(0,random_state=10)._append(df_gen4_multi.sample(0,random_state=10))
df_gen5 = df_gen5_mono.sample(11,random_state=10)
df_gen6 = df_gen6_mono.sample(514,random_state=10)._append(df_gen6_multi.sample(205,random_state=10))
df_gen7 = df_gen7_mono.sample(188,random_state=10)._append(df_gen7_multi.sample(75,random_state=10))
df_gen8 = df_gen8_mono.sample(85,random_state=10)._append(df_gen8_multi.sample(34,random_state=10))

"""Ver 2"""

df_gen1 = df_gen1_mono.sample(85,random_state=20)._append(df_gen1_multi.sample(34,random_state=20))
df_gen2 = df_gen2_mono.sample(282,random_state=20)._append(df_gen2_multi.sample(113,random_state=20))
df_gen3 = df_gen3_mono.sample(91,random_state=20)._append(df_gen3_multi.sample(36,random_state=20))
df_gen4 = df_gen4_mono.sample(54,random_state=20)._append(df_gen4_multi.sample(21,random_state=20))
df_gen5 = df_gen5_mono.sample(75,random_state=20)
df_gen6 = df_gen6_mono.sample(514,random_state=20)._append(df_gen6_multi.sample(205,random_state=20))
df_gen7 = df_gen7_mono.sample(188,random_state=20)._append(df_gen7_multi.sample(75,random_state=20))
df_gen8 = df_gen8_mono.sample(46,random_state=20)._append(df_gen8_multi.sample(19,random_state=20))

"""Ver 3"""

df_gen1 = df_gen1_mono.sample(148,random_state=30)._append(df_gen1_multi.sample(59,random_state=30))
df_gen2 = df_gen2_mono.sample(26,random_state=30)._append(df_gen2_multi.sample(10,random_state=30))
df_gen3 = df_gen3_mono.sample(130,random_state=30)._append(df_gen3_multi.sample(52,random_state=30))
df_gen4 = df_gen4_mono.sample(176,random_state=30)._append(df_gen4_multi.sample(71,random_state=30))
df_gen5 = df_gen5_mono.sample(246,random_state=30)
df_gen6 = df_gen6_mono.sample(0,random_state=30)._append(df_gen6_multi.sample(0,random_state=30))
df_gen7 = df_gen7_mono.sample(123,random_state=30)._append(df_gen7_multi.sample(49,random_state=30))
df_gen8 = df_gen8_mono.sample(164,random_state=30)._append(df_gen8_multi.sample(66,random_state=30))

"""Ver 4"""

df_gen1 = df_gen1_mono.sample(17,random_state=40)._append(df_gen1_multi.sample(7,random_state=40))
df_gen2 = df_gen2_mono.sample(78,random_state=40)._append(df_gen2_multi.sample(30,random_state=40))
df_gen3 = df_gen3_mono.sample(27,random_state=40)._append(df_gen3_multi.sample(11,random_state=40))
df_gen4 = df_gen4_mono.sample(3,random_state=40)._append(df_gen4_multi.sample(1,random_state=40))
df_gen5 = df_gen5_mono.sample(10,random_state=40)
df_gen6 = df_gen6_mono.sample(16,random_state=40)._append(df_gen6_multi.sample(6,random_state=40))
df_gen7 = df_gen7_mono.sample(25,random_state=40)._append(df_gen7_multi.sample(10,random_state=40))
df_gen8 = df_gen8_mono.sample(19,random_state=40)._append(df_gen8_multi.sample(7,random_state=40))

"""Ver 5"""

df_gen1 = df_gen1_mono.sample(505,random_state=50)._append(df_gen1_multi.sample(201,random_state=50))
df_gen2 = df_gen2_mono.sample(385,random_state=50)._append(df_gen2_multi.sample(153,random_state=50))
df_gen3 = df_gen3_mono.sample(488,random_state=50)._append(df_gen3_multi.sample(195,random_state=50))
df_gen4 = df_gen4_mono.sample(534,random_state=50)._append(df_gen4_multi.sample(213,random_state=50))
df_gen5 = df_gen5_mono.sample(665,random_state=50)
df_gen6 = df_gen6_mono.sample(283,random_state=50)._append(df_gen6_multi.sample(113,random_state=50))
df_gen7 = df_gen7_mono.sample(480,random_state=50)._append(df_gen7_multi.sample(192,random_state=50))
df_gen8 = df_gen8_mono.sample(521,random_state=50)._append(df_gen8_multi.sample(208,random_state=50))

df_gen_total = df_gen1._append(df_gen2)._append(df_gen3)._append(df_gen4)._append(df_gen5)._append(df_gen6)._append(df_gen7)._append(df_gen8)

df_gen_total[df_gen_total.NER == '[]']

non_comparative = []
with open('/content/drive/MyDrive/Gen data/Data_ver1/Non_comparative/Non_comparative.txt','r',encoding='utf-8') as file:
    r = file.readline()
    while r != '':
        non_comparative.append(r)
        r = file.readline()
data_frame_non_comparative = {"content": [], "comparative": [], "subject": [], "object": [], "aspect": [], "predicate": [], "label": [], "NER": []}
for i in non_comparative:
    spl = i.split()
    data_frame_non_comparative['content'].append(i)
    data_frame_non_comparative['comparative'].append(0)
    data_frame_non_comparative['subject'].append([])
    data_frame_non_comparative['object'].append([])
    data_frame_non_comparative['aspect'].append([])
    data_frame_non_comparative['predicate'].append([])
    data_frame_non_comparative['label'].append([])
    data_frame_non_comparative['NER'].append([[0 for j in range(len(spl))]])
df_noncomparative = pd.DataFrame(data=data_frame_non_comparative)

df_btc_data = pd.DataFrame(columns=["content", "comparative", "subject", "object", "aspect", "predicate", "label", "NER"])
only_files_btc = [f for f in os.listdir("/content/drive/MyDrive/Gen data/Data_btc") if
              os.path.isfile(os.path.join("/content/drive/MyDrive/Gen data/Data_btc", f))]
for i in only_files_btc:
  df_btc_data=df_btc_data._append(preprocessing_with_BIO('/content/drive/MyDrive/Gen data/Data_btc/'+i))

df_Bao_Phuong = df_read_txt._append(df_gen_total)._append(df_btc_data)._append(df_noncomparative)

df_Bao_Phuong.to_csv('/content/drive/MyDrive/Gen data/Data_ver1/CSV/all_task_ver1.csv', encoding='utf-8')

only_files = [f for f in os.listdir("/content/drive/MyDrive/Gen data/Data_ver1") if
              os.path.isfile(os.path.join("/content/drive/MyDrive/Gen data/Data_ver1", f))]
type_of_compare = ["Non","DIF", "EQL", "SUP+", "SUP-", "SUP", "COM+", "COM-", "COM"]
lst_of_dict = [dict(),dict(),dict(),dict(),dict(),dict(),dict(),dict(),dict()]

df_btc_data[df_btc_data.NER == '[]']

df_btc_data.to_csv('/content/drive/MyDrive/Gen data/Data_btc/CSV/public_and_train1.csv',encoding='utf-8')

stat = [0,0,0,0,0,0,0,0,0]
# df_data = pd.read_csv('/content/drive/MyDrive/Gen data/Data_ver1/CSV/total_shuffled.csv')
df_Bao_Phuong.reset_index(drop=True)
df_Bao_Phuong = df_Bao_Phuong[['content','comparative','subject','object','aspect','predicate','label','NER']]
for i in range(len(df_Bao_Phuong)):
  if isinstance(df_Bao_Phuong['label'].iloc[i], str):
    labels = ast.literal_eval(df_Bao_Phuong['label'].iloc[i])
  else:
    labels = df_Bao_Phuong['label'].iloc[i]
  if len(labels) != 0:
    for j in labels:
      stat[int(j)] += 1
  else:
    stat[0] +=1
print(stat)

import csv

title = ['content', 'comparative', 'subject', 'object',	'aspect',	'predicate','label', 'NER']
with open('/content/drive/MyDrive/Gen data/Data_ver1/CSV/all_task_ver1.csv','w',encoding='utf-8') as f:
  csvwriter = csv.writer(f)
  csvwriter.writerow(title)
  for i in range(len(df_Bao_Phuong)):
    content = df_Bao_Phuong.iloc[i,0]
    comparative = df_Bao_Phuong.iloc[i,1]
    subject = df_Bao_Phuong.iloc[i,2]
    obj = df_Bao_Phuong.iloc[i,3]
    aspect = df_Bao_Phuong.iloc[i,4]
    predicate = df_Bao_Phuong.iloc[i,5]
    label = df_Bao_Phuong.iloc[i,6]
    ner = df_Bao_Phuong.iloc[i,7]
    row = [str(content),str(comparative),str(subject),str(obj),str(aspect),str(predicate),str(label),str(ner)]
    csvwriter.writerow(row)

df = pd.read_csv('/content/drive/MyDrive/Gen data/Data_ver1/CSV/all_task_ver1.csv')

df[df.NER == '[]']

csv = [f for f in os.listdir("/content/drive/MyDrive/Gen data/Data_ver1/CSV") \
  if os.path.isfile(os.path.join("/content/drive/MyDrive/Gen data/Data_ver1/CSV", f))]
df_generate = pd.DataFrame(columns=["content", "comparative", "subject", "object", "aspect", "predicate", "label", "NER"])
for i in csv:
    df_generate = df_generate._append(pd.read_csv("/content/drive/MyDrive/Gen data/Data_ver1/CSV/"+i))

with open('/content/drive/MyDrive/Gen data/Generative/generate_ver1.txt','w',encoding='utf-8') as file:
    for i in range(len(df_gen_total_shuffle)):
        file.write(df_gen_total_shuffle['content'].iloc[i]+'\n')
        s_split = df_gen_total_shuffle['content'].iloc[i].split()
        if isinstance((df_gen_total_shuffle['NER'].iloc[i]),str):
            NER = ast.literal_eval(df_gen_total_shuffle['NER'].iloc[i])
        else:
            NER = df_gen_total_shuffle['NER'].iloc[i]
        if isinstance((df_gen_total_shuffle['subject'].iloc[i]),str):
            subject = ast.literal_eval(df_gen_total_shuffle['subject'].iloc[i])
        else:
            subject = df_gen_total_shuffle['subject'].iloc[i]
        if isinstance((df_gen_total_shuffle['object'].iloc[i]),str):
            object = ast.literal_eval(df_gen_total_shuffle['object'].iloc[i])
        else:
            object = df_gen_total_shuffle['object'].iloc[i]
        if isinstance((df_gen_total_shuffle['aspect'].iloc[i]),str):
            aspect = ast.literal_eval(df_gen_total_shuffle['aspect'].iloc[i])
        else:
            aspect = df_gen_total_shuffle['aspect'].iloc[i]
        if isinstance((df_gen_total_shuffle['predicate'].iloc[i]),str):
            predicate = ast.literal_eval(df_gen_total_shuffle['predicate'].iloc[i])
        else:
            predicate = df_gen_total_shuffle['predicate'].iloc[i]
        if isinstance((df_gen_total_shuffle['label'].iloc[i]),str):
            label = ast.literal_eval(df_gen_total_shuffle['label'].iloc[i])
        else:
            label = df_gen_total_shuffle['label'].iloc[i]
        list_of_json = []
        for j in range(len(label)):
            list_subject = []
            subject_index = []
            list_object = []
            object_index = []
            list_predicate = []
            predicate_index = []
            list_aspect = []
            aspect_index = []
            for k in range(len(NER[j])):
                if NER[j][k] == 1 or NER[j][k] == 2:
                    list_subject.append(s_split[k])
                    subject_index.append(k+1)
                elif NER[j][k] == 3 or NER[j][k] == 4:
                    list_object.append(s_split[k])
                    object_index.append(k+1)
                elif NER[j][k] == 5 or NER[j][k] == 6:
                    list_aspect.append(s_split[k])
                    aspect_index.append(k+1)
                elif NER[j][k] == 7 or NER[j][k] == 8:
                    list_predicate.append(s_split[k])
                    predicate_index.append(k+1)
            if subject[j] == ' '.join(list_subject):
                for k in range(len(list_subject)):
                    list_subject[k] = str(subject_index[k])+'&&'+list_subject[k]
            else:
                print(subject[j],list_subject,NER[j])
            if object[j] == ' '.join(list_object):
                for k in range(len(list_object)):
                    list_object[k] = str(object_index[k])+'&&'+list_object[k]
            else:
                print(object[j],list_object,NER[j])
            if predicate[j] == ' '.join(list_predicate):
                for k in range(len(list_predicate)):
                    list_predicate[k] = str(predicate_index[k])+'&&'+list_predicate[k]
            else:
                print(predicate[j],list_predicate,NER[j])
            if aspect[j] == ' '.join(list_aspect):
                for k in range(len(list_aspect)):
                    list_aspect[k] = str(aspect_index[k])+'&&'+list_aspect[k]
            else:
                print(aspect[j],list_aspect,NER[j])
            json_total = '{\"subject\": '+list_subject.__str__()+', \"object\": '+list_object.__str__()+', \"aspect\": '+list_aspect.__str__()+', \"predicate\": '+list_predicate.__str__()+', \"label\": \"'+type_of_compare[label[j]-1] +'\"}'
            list_of_json.append(json_total)
        for j in list_of_json:
            file.write(j+'\n')
        file.write('\n')

df_data_ver = pd.read_csv('/content/drive/MyDrive/Gen data/Data_ver1/CSV/all_task_ver1.csv')

df_data_ver[df_data_ver.NER == '[]']

def append_ner(lst):
  if isinstance(lst,str):
    new_lst = ast.literal_eval(lst)
  else:
    new_lst = lst
  true_ner = new_lst[0]
  for i in range(len(new_lst)):
    for j in range(len(new_lst[i])):
      if new_lst[i][j] in [5,6,7,8]:
        true_ner[j] = new_lst[i][j]
  return str(true_ner)

df_data_ver['NER'] = df_data_ver['NER'].map(lambda x: append_ner(x))

df_data_ver['NER']

df_data_ver.to_csv('/content/drive/MyDrive/Gen data/Data_ver1/CSV/all_task_ver1.csv')

"""# Down is redundent

"""

count = 0
for i in range(len(df_data_ver['NER'])):
  if df_data_ver['NER'].iloc[i] == 0:
    count += 1
print(count)

df_gen_total = df_gen_data_txt._append(df_generate)
df_gen_total_shuffle = df_gen_total.sample(len(df_gen_total),random_state=50)
df_gen_data_txt.to_csv('/content/drive/MyDrive/Gen data/Data_ver3/CSV/total_shuffled.csv')

df_data_ver_1['NER'].iloc[0]

exp = eval(df_data_ver_1.iloc[170,8])
for i in exp:
  print(i)

print(append_ner(exp))

df_gen1.to_csv('Gen_data/CSV/DIF.csv')
df_gen2.to_csv('Gen_data/CSV/EQL.csv')
df_gen3.to_csv('Gen_data/CSV/SUP_plus.csv')
df_gen4.to_csv('Gen_data/CSV/SUP_minus.csv')
df_gen5.to_csv('Gen_data/CSV/SUP.csv')
df_gen6.to_csv('Gen_data/CSV/COM_plus.csv')
df_gen7.to_csv('Gen_data/CSV/COM_minus.csv')
df_gen8.to_csv('Gen_data/CSV/COM.csv')

df_gen_total_shuffle

dct_reverse_convert = df_gen_total_shuffle.to_dict(orient='list')

print(stat)

"DIF", "EQL", "SUP+", "SUP-", "SUP", "COM+", "COM-", "COM"

only_files = [f for f in os.listdir("Gen_data") if
              os.path.isfile(os.path.join("Gen_data", f))]
data_frame = {"content": [], "comparative": [], "subject": [], "object": [], "aspect": [], "predicate": [], "label": [], "NER": []}



df_merge_data = pd.read_csv('merge.csv', encoding='utf-8')
df_merge_data.drop('Unnamed: 0', axis=1)

df_comparative = df_merge_data[df_merge_data.comparative == 1]
df_gen_and_org = df_gen_total_shuffle._append(df_comparative)
df_gen_and_org_shuffle = df_gen_and_org.sample(len(df_gen_and_org),random_state=50)
df_gen_and_org_shuffle.to_csv('Gen_data/gen_and_org.csv')

df_combined = df_merge_data._append(df_noncomparative)
df_combined.drop('Unnamed: 0',axis=1)
df_total = df_combined._append(df_gen_total_shuffle)
df_total.drop('Unnamed: 0',axis=1)
df_total_shuffle = df_total.sample(len(df_total),random_state=1)

df_total_shuffle.to_csv('Final_data_ver3.csv')

df_shuffle.to_csv('Final_data_ver1.csv')

df_comparative = df_total[df_total.comparative == 1]

with open('New Data/resource.txt', 'w', encoding='utf-8') as file:
    for i in resource.keys():
        file.write(i+'\n')
        for j in resource[i][0]:
            file.write(j.__str__()+'\n')
        file.write('\n')

df_data = pd.DataFrame(data_frame)

df_compare = df_data[df_data.comparative == 1]
df_compare

random.seed(100)
counter = 0
stat = [0,0,0,0,0,0,0,0,0,0]
dct_split = {'content':[],'subject':[],'label':[],'object':[],'aspect':[],'predicate':[], 'NER':[]}
lst = df_shuffle_tottal.to_dict(orient= "list")
pred_stat = dict()
aspect_stat = dict()
overlap = dict()
for i in range(len(lst['content'])):
    subject = lst['subject'][i]
    labels = lst['label'][i]
    object = lst['object'][i]
    aspect = lst['aspect'][i]
    predicate = lst['predicate'][i]
    ner = lst['NER'][i]
    for j in range(len(labels)):
        n_subject = subject[j]
        n_object = object[j]
        n_aspect = aspect[j]
        pred = predicate[j]
        n_ner = ner[j]
        dct_split['content'].append(lst['content'][i])
        dct_split['subject'].append(n_subject)
        dct_split['object'].append(n_object)
        dct_split['aspect'].append(n_aspect)
        dct_split['predicate'].append(pred)
        dct_split['label'].append(labels[j])
        dct_split['NER'].append(n_ner)
        # vocab['subject'].add(n_subject)
        # vocab['object'].add(n_object)
        # stat[labels[j]] += 1
        # if n_aspect in vocab['aspect'].keys():
        #     if labels[j] not in vocab['aspect'][n_aspect]:
        #         vocab['aspect'][n_aspect].append(labels[j])
        # else:
        #     vocab['aspect'][n_aspect]=[labels[j]]
        # if pred in vocab['predicate'].keys():
        #     if labels[j] not in vocab['predicate'][pred]:
        #         vocab['predicate'][pred].append(labels[j])
        # else:
        #     vocab['predicate'][pred]=[labels[j]]
        # # Predicate statistics on labels
        # if pred in pred_stat.keys():
        #     pred_stat[pred][labels[j]] += 1
        # else:
        #     pred_stat[pred]=[0,0,0,0,0,0,0,0,0]
        #     pred_stat[pred][labels[j]]+=1
        # # Overallping labels
        # if n_object in pred and n_object != '':
        #     if pred in overlap.keys():
        #         overlap[pred].append('ob_'+n_object)
        #     else:
        #         overlap[pred] = ['ob_'+n_object]
        # if n_subject in pred and n_subject != '':
        #     if pred in overlap.keys():
        #         overlap[pred].append('sub_'+n_subject)
        #     else:
        #         overlap[pred] = ['sub_'+n_subject]
        # if n_aspect in pred and n_aspect != '':
        #     if pred in overlap.keys():
        #         overlap[pred].append('asp_'+n_aspect)
        #     else:
        #         overlap[pred] = ['asp_'+n_aspect]
        # # Asp sat
        # if n_aspect in aspect_stat.keys():
        #     aspect_stat[n_aspect][labels[j]] += 1
        # else:
        #     aspect_stat[n_aspect]=[0,0,0,0,0,0,0,0,0]
        #     aspect_stat[n_aspect][labels[j]]+=1
df_splited = pd.DataFrame(data=dct_split)

df_splited.to_csv('Final_data_ver2.csv')

for i in dct_split['content']:
    if 'orig' in i:
        print(i)

def finder(n, id, se, w):
    sen = se.split()
    start = -1
    end = -1
    for s in range(len(n)):
        if n[s] == id and (start == -1 and end == -1):
            start = s
            end = s
        elif n[s] == id+1 and start != -1 and end != -1:
            end = s
        elif n[s] != id+1 and start != -1 and end != -1:
            if " ".join(sen[start:end+1]) == w:
                break
            else:
                start = -1
                end = -1
                s -= 1
    return start, end
content = df_splited.to_dict(orient='list')['content']
data = {"content": [], "subject": [], "object": [], "aspect": [], "predicate": [], "label": [], "NER": []}
for k in range(1,9):
    df_comp = df_splited[df_splited['label'] == k]
    i = 0
    count = 0
    while stat[k]+count <= 600:
        sentence = df_comp.iloc[i]['content']
        su = df_comp.iloc[i]['subject']
        ob = df_comp.iloc[i]['object']
        asp = df_comp.iloc[i]['aspect']
        pr = df_comp.iloc[i]['predicate']
        n = df_comp.iloc[i]['NER']
        new_subject = ""
        new_aspect = ""
        new_object = ""
        lst_pred = []
        lst_asp = []
        for j in vocab['predicate']:
            if k in vocab['predicate'][j]:
                lst_pred.append(j)
        for j in vocab['aspect']:
            if k in vocab['aspect'][j]:
                lst_asp.append(j)
        new_predicate = random.choice(list(lst_pred))
        substitute = new_predicate.split()
        checkpoint = finder(n,7,sentence,pr)
        s = sentence.split()
        s = s[:checkpoint[0]]+substitute+s[checkpoint[1]+1:]
        sentence = " ".join(s)
        rep = [7]+[8 for p in range(len(substitute)-1)]
        n = n[:checkpoint[0]]+rep+ n[checkpoint[1]+1:]
        if len(sentence.split()) != len(n):
            generror.append(sentence+','+n.__str__()+','+str(k)+','+pr+','+new_predicate+',pr,'+str(checkpoint))
        if asp != '' and asp in sentence:
            new_aspect = random.choice(lst_asp)
            substitute = new_aspect.split()
            checkpoint = finder(n,5,sentence,asp)
            s = sentence.split()
            s = s[:checkpoint[0]]+substitute+s[checkpoint[1]+1:]
            sentence = " ".join(s)
            if new_aspect != '':
                rep = [5]+[6 for p in range(len(substitute)-1)]
            else:
                rep = []
            n = n[:checkpoint[0]]+rep+ n[checkpoint[1]+1:]
            if len(sentence.split()) != len(n):
                generror.append(sentence+','+n.__str__()+','+str(k)+','+asp+','+new_aspect+',asp,'+str(checkpoint))
        if su != '' and su in sentence:
            new_subject = random.choice(list(vocab['subject']))
            substitute = new_subject.split()
            checkpoint = finder(n,1,sentence,su)
            s = sentence.split()
            s = s[:checkpoint[0]]+substitute+s[checkpoint[1]+1:]
            sentence = " ".join(s)
            if new_subject != '':
                rep = [1]+[2 for p in range(len(substitute)-1)]
            else:
                rep = []
            n = n[:checkpoint[0]]+rep+ n[checkpoint[1]+1:]
            if len(sentence.split()) != len(n):
                generror.append(sentence+','+n.__str__()+','+str(k)+','+su+','+new_subject+',sub,'+str(checkpoint))
        if ob != '' and ob in sentence:
            new_object = random.choice(list(vocab['object']))
            substitute = new_object.split()
            checkpoint = finder(n,3,sentence,ob)
            s = sentence.split()
            s = s[:checkpoint[0]]+substitute+s[checkpoint[1]+1:]
            sentence = " ".join(s)
            if new_object != '':
                rep = [3]+[4 for p in range(len(substitute)-1)]
            else:
                rep = []
            n = n[:checkpoint[0]]+rep+ n[checkpoint[1]+1:]
            if len(sentence.split()) != len(n):
                generror.append(sentence+','+n.__str__()+','+str(k)+','+ob+','+new_object+',ob,'+str(checkpoint))
        if sentence not in content:
            content.append(sentence)
            data['content'].append(sentence)
            data['NER'].append(n)
            data['label'].append(k)
            data['subject'].append(new_subject)
            data['object'].append(new_object)
            data['aspect'].append(new_aspect)
            data['predicate'].append(new_predicate)
            count += 1
        i += 1
        if i == stat[k]:
            i = 0



# dct_split['content'].extend(data['content'])
# dct_split['aspect'].extend(data['aspect'])
# dct_split['subject'].extend(data['subject'])
# dct_split['object'].extend(data['object'])
# dct_split['NER'].extend(data['NER'])
# dct_split['predicate'].extend(data['predicate'])
# dct_split['label'].extend(data['label'])
df_total = pd.DataFrame(data=data)

df_shuffle = df_total.sample(n=len(df_total),random_state=100)

df_shuffle.to_csv('sample.csv',encoding='utf-8')

print(generror)

"""Khúc sau là statistic một xíu của data thì m muốn xem qua thì xem"""

'Và Snapdragon 450   không nhanh và mạnh như những điện thoại tầm trung hay cao cấp .'.split()

print(stat)

print(vocab['aspect']['thời lượng pin'])

new_pred = []
print(len(vocab['aspect']))
for i in aspect_stat.keys():
    if aspect_stat[i][4] > 0:
        print(i,aspect_stat[i])
# for j in vocab['aspect']:
#     print(j,vocab['aspect'][j])

counter = 0
counter1 = 0
counter2 = 0
counter3 = 0
counter4 = 0
counter5 = 0
for i in pred_stat:
    if sum(pred_stat[i]) < 2:
        counter += 1
    if sum(pred_stat[i]) > 1 and sum(pred_stat[i]) < 5:
        counter1 += 1
    if sum(pred_stat[i]) > 4 and sum(pred_stat[i]) < 10:
        counter2 += 1
    if sum(pred_stat[i]) > 9 and sum(pred_stat[i]) < 20:
        counter3 += 1
    if sum(pred_stat[i]) > 19 and sum(pred_stat[i]) < 30:
        counter4 += 1
    if sum(pred_stat[i]) >= 30:
        counter5 += 1
print(counter)
print(counter1)
print(counter2)
print(counter3)
print(counter4)
print(counter5)

for i in range(len(data['content'])-1,-1,-1):
    if len(data['content'][i].split()) != len(data['NER'][i]):
        data['content'].pop(i)
        data['NER'].pop(i)
        data['label'].pop(i)

counter = [0,0,0,0,0,0,0,0,0]
count = 0
for i in range(len(data['content'])):
    counter[data['label'][i]] += 1
print(counter)
print(stat)

df_prev = pd.read_csv('Data/total.csv',encoding='utf-8')
dct_prev = df_prev[df_prev['comparative'] == 1].to_dict(orient='list')
counter = 0
dct_total = {'content':dct_split['content'],'NER':dct_split['NER'],'label':dct_split['label']}
for i in range(len(data['content'])):
    if stat[data['label'][i]] > 600:
        continue
    dct_total['content'].append(data['content'][i])
    dct_total['NER'].append(data['NER'][i])
    dct_total['label'].append(data['label'][i])
    stat[data['label'][i]]+=1

df_generate = pd.DataFrame(data=dct_total)
df_shuffled = df_generate.sample(len(df_generate), random_state=1)
df_shuffled.to_csv('Data/combined_BIO.csv')

import csv
# name of csv file
rows = []
filename = "combined.csv"
with open(filename, 'w', encoding="utf-8") as csvfile:
    csvwriter = csv.writer(csvfile)
    csvwriter.writerow(["Content","NER","Label"])
    for i in range(len(dct_total['content'])):
        row = [dct_total['content'][i]]
        row.append(dct_total['NER'][i])
        row.append(dct_total['label'][i])
        rows.append(row)
    csvwriter.writerows(rows)

d = pd.read_csv('combined.csv',encoding='utf-8')
d.head()